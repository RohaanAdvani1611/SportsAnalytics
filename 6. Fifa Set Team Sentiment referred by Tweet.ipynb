{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT TITLE: FINDING THE MOST INTERESTING TWEETS AND FAN SENTIMENT DURING A GAME TELECAST IN REALTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 6: SET TEAM / SENTIMENT TWEET REFERS TO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\rohaan\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rohaan\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\rohaan\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\rohaan\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohaan\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\rohaan\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1076)'))) - skipping\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>ExtraMins</th>\n",
       "      <th>Team1</th>\n",
       "      <th>Team2</th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Event</th>\n",
       "      <th>MatchNo</th>\n",
       "      <th>EventID</th>\n",
       "      <th>EventHr</th>\n",
       "      <th>EventMnt</th>\n",
       "      <th>Time</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>START</td>\n",
       "      <td>1</td>\n",
       "      <td>1101</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>17:00</td>\n",
       "      <td>30-06-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>GIROUD</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>1102</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>17:01</td>\n",
       "      <td>30-06-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>POGBA</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>1103</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>17:03</td>\n",
       "      <td>30-06-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>MATUIDI</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>1104</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>17:03</td>\n",
       "      <td>30-06-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>MASCHERANO</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>1105</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>17:08</td>\n",
       "      <td>30-06-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>CROATIA</td>\n",
       "      <td>VRSALJKO</td>\n",
       "      <td>CROATIA</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>16</td>\n",
       "      <td>16230</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>19:34</td>\n",
       "      <td>15-07-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>CROATIA</td>\n",
       "      <td>VRSALJKO</td>\n",
       "      <td>CROATIA</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>16</td>\n",
       "      <td>16231</td>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>19:35</td>\n",
       "      <td>15-07-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>CROATIA</td>\n",
       "      <td>VRSALJKO</td>\n",
       "      <td>CROATIA</td>\n",
       "      <td>YELLOW CARD</td>\n",
       "      <td>16</td>\n",
       "      <td>16232</td>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>19:36</td>\n",
       "      <td>15-07-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>CROATIA</td>\n",
       "      <td>FEKIR</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>16</td>\n",
       "      <td>16233</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "      <td>19:37</td>\n",
       "      <td>15-07-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>CROATIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>END</td>\n",
       "      <td>16</td>\n",
       "      <td>16234</td>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>19:39</td>\n",
       "      <td>15-07-2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1466 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Timestamp  ExtraMins   Team1      Team2      Player       Team  \\\n",
       "0             0          0  FRANCE  ARGENTINA         NaN        NaN   \n",
       "1             1          0  FRANCE  ARGENTINA      GIROUD     FRANCE   \n",
       "2             3          0  FRANCE  ARGENTINA       POGBA     FRANCE   \n",
       "3             3          0  FRANCE  ARGENTINA     MATUIDI     FRANCE   \n",
       "4             8          0  FRANCE  ARGENTINA  MASCHERANO  ARGENTINA   \n",
       "...         ...        ...     ...        ...         ...        ...   \n",
       "1461         90          1  FRANCE    CROATIA    VRSALJKO    CROATIA   \n",
       "1462         90          2  FRANCE    CROATIA    VRSALJKO    CROATIA   \n",
       "1463         90          3  FRANCE    CROATIA    VRSALJKO    CROATIA   \n",
       "1464         90          4  FRANCE    CROATIA       FEKIR     FRANCE   \n",
       "1465         90          6  FRANCE    CROATIA         NaN        NaN   \n",
       "\n",
       "            Event  MatchNo  EventID  EventHr  EventMnt   Time        Date  \n",
       "0           START        1     1101       17         0  17:00  30-06-2018  \n",
       "1            FOUL        1     1102       17         1  17:01  30-06-2018  \n",
       "2            FOUL        1     1103       17         3  17:03  30-06-2018  \n",
       "3            FOUL        1     1104       17         3  17:03  30-06-2018  \n",
       "4            FOUL        1     1105       17         8  17:08  30-06-2018  \n",
       "...           ...      ...      ...      ...       ...    ...         ...  \n",
       "1461         FOUL       16    16230       19        34  19:34  15-07-2018  \n",
       "1462         FOUL       16    16231       19        35  19:35  15-07-2018  \n",
       "1463  YELLOW CARD       16    16232       19        36  19:36  15-07-2018  \n",
       "1464         FOUL       16    16233       19        37  19:37  15-07-2018  \n",
       "1465          END       16    16234       19        39  19:39  15-07-2018  \n",
       "\n",
       "[1466 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfEv = pd.read_csv('events_proc.csv')\n",
    "dfEv.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "dfEv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_team_from_tweet(fn):\n",
    "    ################################################################\n",
    "    # STEP 1: INITIALIZE ARRAY'S TO COLLECT EVENT DATA INSIGHTS:\n",
    "    # A. PLAYER TEAM SET - SET OF ALL PLAYER AND TEAMS THEY BELONG TO\n",
    "    # B. ABB SET - SET OF ALL ABB'S, TWEET CONTENT INCLUDES ABB'S\n",
    "    # C. KEY WORD SET - SET OF PLAYER NAMES, TEAM NAMES, ABB'S, COMBINED ABB'S\n",
    "    # D. TEAM ABB SET - RELATES ABB TO TEAM NAME\n",
    "    # E. TEAM-TEAM ABB SET - REALTES COMBINED ABB TO BOTH TEAM NAMES\n",
    "    # F. TEAM LIST\n",
    "\n",
    "    # Key Word Array\n",
    "    kw_arr = []\n",
    "    # Keyword Set\n",
    "    kw_set = []\n",
    "\n",
    "    # Player Team Array\n",
    "    pt_arr = []\n",
    "    # Player Team Set\n",
    "    pt_set = []\n",
    "\n",
    "    # Abbreviation Array\n",
    "    abb_arr = []\n",
    "    # Abbreviation Set\n",
    "    abb_set = []\n",
    "\n",
    "    # Team Abbreviation Array\n",
    "    ta_arr = []\n",
    "    # Team Abbreviation Set\n",
    "    ta_set = []\n",
    "\n",
    "    # Team-Team Abbreviation Array\n",
    "    tta_arr = []\n",
    "    # Team-Team Abbreviation Set\n",
    "    tta_set = []\n",
    "\n",
    "    # Team List\n",
    "    team_list = []\n",
    "    # Team Set\n",
    "    team_set = []\n",
    "\n",
    "    #################################################################################\n",
    "    # STEP 2: BUILD THE RESPECTIVE SETS\n",
    "\n",
    "    dfEv.Player = dfEv.Player.fillna('')\n",
    "    dfEv.Team = dfEv.Team.fillna('')\n",
    "\n",
    "    for i in range(1466):\n",
    "        if dfEv['Player'][i] != '':\n",
    "            pt_arr.append([dfEv['Player'][i].lower(), dfEv['Team'][i].lower()])\n",
    "            ta_arr.append([dfEv['Team'][i].lower(), dfEv['Team'][i][:3].lower()])\n",
    "            team_list.append(dfEv['Team'][i].lower())\n",
    "            kw_arr.append(dfEv['Player'][i].lower())\n",
    "            kw_arr.append(dfEv['Team'][i].lower())\n",
    "\n",
    "    for i in range(1466):\n",
    "        a = dfEv['Team1'][i][:3].lower()\n",
    "        b = dfEv['Team2'][i][:3].lower()\n",
    "        c = a + b\n",
    "        abb_arr.append([a, b, c])\n",
    "        tta_arr.append([dfEv['Team1'][i].lower(), dfEv['Team2'][i].lower(), c])\n",
    "        kw_arr.append(a)\n",
    "        kw_arr.append(b)\n",
    "        kw_arr.append(c)\n",
    "\n",
    "\n",
    "    for i in pt_arr:\n",
    "        if i not in pt_set:\n",
    "            pt_set.append(i)\n",
    "\n",
    "    for i in abb_arr:\n",
    "        if i not in abb_set:\n",
    "            abb_set.append(i)\n",
    "\n",
    "    for i in kw_arr:\n",
    "        if i not in kw_set:\n",
    "            kw_set.append(i)\n",
    "\n",
    "    for i in ta_arr:\n",
    "        if i not in ta_set:\n",
    "            ta_set.append(i)\n",
    "\n",
    "    for i in tta_arr:\n",
    "        if i not in tta_set:\n",
    "            tta_set.append(i)\n",
    "\n",
    "    for i in team_list:\n",
    "        if i not in team_set:\n",
    "            team_set.append(i)\n",
    "\n",
    "    ################################################################################\n",
    "    # STEP 3: COLLECT BAG OF WORDS FROM TWEET DATA, HASHTAG DATA, USER MENTIONED NAMES / IDS\n",
    "\n",
    "    df = pd.read_csv(fn)\n",
    "    df.drop(['Unnamed: 0', 'HTs', 'UMN', 'UMID'], axis = 1, inplace = True)\n",
    "\n",
    "    df1 = df[\"Tweet_Extra\"]\n",
    "    # Tweet Exra Content Array - Contains HTs, UMN, UMID data\n",
    "    te = []\n",
    "    for i in df1:\n",
    "        x = i.split('[')\n",
    "        y = x[1].split(']')\n",
    "        z = y[0].split(\"'\")\n",
    "        te.append(z)\n",
    "    for i in te:\n",
    "        i.remove('')\n",
    "    for i in te:\n",
    "        for j in i:\n",
    "            if any(c.isalpha() for c in j):\n",
    "                pass\n",
    "            else:\n",
    "                i.remove(j)\n",
    "\n",
    "    df1 = df[\"Tweet_Proc\"]\n",
    "    # Tweet Content Array - Contains Tweet Data\n",
    "    tc = []\n",
    "    for i in df1:\n",
    "        x = i.split('[')\n",
    "        y = x[1].split(']')\n",
    "        z = y[0].split(\"'\")\n",
    "        tc.append(z)\n",
    "    for i in tc:\n",
    "        i.remove('')\n",
    "    for i in tc:\n",
    "        for j in i:\n",
    "            if any(c.isalpha() for c in j):\n",
    "                pass\n",
    "            else:\n",
    "                i.remove(j)\n",
    "\n",
    "    ############################################################################################################\n",
    "    # STEP 4: FIND INTERSECTIONS OF KEYWORDS AND TWEETS TO DRAW INSIGHTS AS TO WHICH TEAM IS BEING REFERRED TO:\n",
    "\n",
    "    # ARR1 - INTERSECTION OF KEYWORDS AND TWEET CONTENT\n",
    "    arr1 = [] \n",
    "    set_A = set(kw_set)\n",
    "    for i in range(len(tc)):\n",
    "        set_B = set(tc[i])\n",
    "        arr1.append(list(set_A.intersection(set_B)))\n",
    "\n",
    "    # ARR2 - INTERSECTION OF KEYWORDS AND TWEET EXTRAS\n",
    "    arr2 = []\n",
    "    for i in range(len(te)):\n",
    "        set_C = set(te[i])\n",
    "        arr2.append(list(set_A.intersection(set_C)))\n",
    "\n",
    "    ##############################################################################################\n",
    "    # STEP 5: IF PLAYER NAME / ABB KEYWORDS ARE FOUND ADD TEAM NAME ALSO\n",
    "    for i in arr1:\n",
    "        for j in i:\n",
    "            for k in pt_set:\n",
    "                if k[0] == j:\n",
    "                    i.append(k[1])\n",
    "\n",
    "    for i in arr2:\n",
    "        for j in i:\n",
    "            for k in pt_set:\n",
    "                if k[0] == j:\n",
    "                    i.append(k[1])\n",
    "\n",
    "    for i in arr1:\n",
    "        for j in i:\n",
    "            for k in ta_set:\n",
    "                if k[1] == j:\n",
    "                    i.append(k[0])\n",
    "\n",
    "    for i in arr2:\n",
    "        for j in i:\n",
    "            for k in ta_set:\n",
    "                if k[1] == j:\n",
    "                    i.append(k[0])\n",
    "\n",
    "    for i in arr1:\n",
    "        for j in i:\n",
    "            for k in tta_set:\n",
    "                if k[2] == j:\n",
    "                    i.append(k[0])\n",
    "                    i.append(k[1])\n",
    "\n",
    "    for i in arr2:\n",
    "        for j in i:\n",
    "            for k in tta_set:\n",
    "                if k[2] == j:\n",
    "                    i.append(k[0])\n",
    "                    i.append(k[1])\n",
    "\n",
    "    #############################################################################\n",
    "    # STEP 6: COMBINE DATA REMOVE KEYWORDS OTHER THAN DERIVED TEAM NAMES\n",
    "\n",
    "    # ARR3 - COMBINE ARR1 AND ARR2 CONTENT\n",
    "    arr3 = []\n",
    "    for i in range(len(arr1)):\n",
    "        arr3.append(arr1[i] + arr2[i])\n",
    "\n",
    "    # ARR4 - WILL CONTAIN ONLY TEAM REFERRED TO\n",
    "    arr4 = []\n",
    "    for i in arr3:\n",
    "        tmp = []\n",
    "        for j in i:\n",
    "            if j in team_set:\n",
    "                tmp.append(j)\n",
    "        arr4.append(tmp)\n",
    "\n",
    "    df['TeamsRef'] = arr4\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(df):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    df.Tweet = df.Tweet.fillna('')\n",
    "    rows = df.shape[0]\n",
    "    df['PosSent'] = 0\n",
    "    df['NegSent'] = 0\n",
    "    df['NeuSent'] = 0\n",
    "\n",
    "    for i in range(rows):\n",
    "        sentiment_dict = sid_obj.polarity_scores(df['Tweet'][i])\n",
    "        df['PosSent'][i] = sentiment_dict['pos']*100\n",
    "        df['NegSent'][i] = sentiment_dict['neg']*100\n",
    "        df['NeuSent'][i] = sentiment_dict['neu']*100\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df30 = set_team_from_tweet('tweet_3006.csv')\n",
    "df30 = sentiment_scores(df30)\n",
    "df30.to_csv('tweet_new_3006.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df01 = set_team_from_tweet('tweet_0107.csv')\n",
    "df01 = sentiment_scores(df01)\n",
    "df01.to_csv('tweet_new_0107.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df02 = set_team_from_tweet('tweet_0207.csv')\n",
    "df02 = sentiment_scores(df02)\n",
    "df02.to_csv('tweet_new_0207.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df04 = set_team_from_tweet('tweet_0407.csv')\n",
    "df04 = sentiment_scores(df04)\n",
    "df04.to_csv('tweet_new_0407.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = set_team_from_tweet('tweet_1007.csv')\n",
    "df10 = sentiment_scores(df10)\n",
    "df10.to_csv('tweet_new_1007.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = set_team_from_tweet('tweet_1107.csv')\n",
    "df11 = sentiment_scores(df11)\n",
    "df11.to_csv('tweet_new_1107.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = set_team_from_tweet('tweet_1207.csv')\n",
    "df12 = sentiment_scores(df12)\n",
    "df12.to_csv('tweet_new_1207.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15 = set_team_from_tweet('tweet_1507.csv')\n",
    "df15 = sentiment_scores(df15)\n",
    "df15.to_csv('tweet_new_1507.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16 = set_team_from_tweet('tweet_1607.csv')\n",
    "df16 = sentiment_scores(df16)\n",
    "df16.to_csv('tweet_new_1607.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
